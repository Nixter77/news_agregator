#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Streamlit RSS Aggregator
Ð—Ð°Ð¿ÑƒÑÐº:  streamlit run app.py
"""

import asyncio, logging, re
from typing import Dict, List

import aiohttp
import feedparser
import streamlit as st

# ---------------------------------------------------------------------------------
# 1) ÐšÐ¾Ð½ÑÑ‚Ð°Ð½Ñ‚Ñ‹
# ---------------------------------------------------------------------------------
FEEDS = {
    "BBC":          "https://feeds.bbci.co.uk/news/world/rss.xml",
    "Guardian":     "https://www.theguardian.com/world/rss",
    "Reuters":      "https://feeds.reuters.com/reuters/worldNews",
    "Al-Jazeera":   "https://www.aljazeera.com/xml/rss/all.xml",
    "CNN":          "https://rss.cnn.com/rss/edition_world.rss",
}

FALLBACK: Dict[str, str] = {
    FEEDS["Reuters"]:
        "https://news.google.com/rss/search?q=source:Reuters+when:7d&hl=en&gl=US&ceid=US:en"
}

HEADERS = {
    "User-Agent": ("Mozilla/5.0 (Macintosh) AppleWebKit/537.36 "
                   "(KHTML, like Gecko) Chrome/124.0 Safari/537.36"),
    "Accept": "application/rss+xml,application/xml;q=0.9,*/*;q=0.8",
    "Referer": "https://www.google.com/",
}

MAX_RETRIES, RETRY_SLEEP = 3, 4
CONCURRENCY              = 5

logging.getLogger("asyncio").setLevel(logging.CRITICAL)  # Ñ‚Ð¸ÑˆÐ¸Ð½Ð° Ð¾Ñ‚ aiohttp

# ---------------------------------------------------------------------------------
# 2) ÐÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ð°Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð»ÐµÐ½Ñ‚Ñ‹
# ---------------------------------------------------------------------------------
SEM = asyncio.Semaphore(CONCURRENCY)
RX_HTML = re.compile(r"html", re.I)

async def _fetch(session: aiohttp.ClientSession, url: str) -> str:
    real = FALLBACK.get(url, url)
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            async with SEM:
                async with session.get(real, headers=HEADERS, timeout=25) as r:
                    r.raise_for_status()
                    if RX_HTML.search(r.headers.get("Content-Type", "")):
                        raise aiohttp.ClientError("HTML Ð²Ð¼ÐµÑÑ‚Ð¾ RSS")
                    return await r.text()
        except Exception as e:
            if attempt == MAX_RETRIES:
                raise
            await asyncio.sleep(RETRY_SLEEP)

async def _grab_one(session: aiohttp.ClientSession, name: str, url: str) -> List[Dict]:
    try:
        raw = await _fetch(session, url)
    except Exception as e:
        return [{"title": f"ðŸš« ÐžÑˆÐ¸Ð±ÐºÐ°: {e}", "link": "", "published": ""}]

    feed = feedparser.parse(raw)
    return [
        {
            "title":      e.get("title", "â€”"),
            "link":       e.get("link", ""),
            "published":  e.get("published", ""),
        }
        for e in feed.entries
    ]

async def _collect(feeds_subset: Dict[str, str]) -> Dict[str, List[Dict]]:
    async with aiohttp.ClientSession() as sess:
        tasks = [_grab_one(sess, name, url) for name, url in feeds_subset.items()]
        results = await asyncio.gather(*tasks)
    return dict(zip(feeds_subset.keys(), results))

# ---------------------------------------------------------------------------------
# 3) Streamlit-Ð¾Ð±Ñ‘Ñ€Ñ‚ÐºÐ°
# ---------------------------------------------------------------------------------
@st.cache_data(ttl=600, show_spinner=False)   # 10 Ð¼Ð¸Ð½ÑƒÑ‚ ÐºÑÑˆ
def load_feeds(feeds_subset: Dict[str, str]) -> Dict[str, List[Dict]]:
    return asyncio.run(_collect(feeds_subset))

# ---------------------------------------------------------------------------------
# 4) UI
# ---------------------------------------------------------------------------------
st.set_page_config(page_title="ðŸŒ RSS Aggregator", layout="wide")
st.title("ðŸŒ RSS-Ð°Ð³Ñ€ÐµÐ³Ð°Ñ‚Ð¾Ñ€ Ð¼ÐµÐ¶Ð´ÑƒÐ½Ð°Ñ€Ð¾Ð´Ð½Ñ‹Ñ… Ð½Ð¾Ð²Ð¾ÑÑ‚ÐµÐ¹")

st.sidebar.header("âš™ï¸  ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹")
selected = st.sidebar.multiselect(
    "ÐšÐ°ÐºÐ¸Ðµ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ Ñ‚ÑÐ½ÑƒÑ‚ÑŒ?",
    list(FEEDS.keys()),
    default=list(FEEDS.keys())[:3],
)
n_show = st.sidebar.slider("Ð¡ÐºÐ¾Ð»ÑŒÐºÐ¾ ÑÑ‚Ð°Ñ‚ÐµÐ¹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ Ð½Ð° Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº?",
                           3, 50, 10, step=1)
refresh = st.sidebar.button("ðŸ”„ ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ (Ð¾Ð±Ð¾Ð¹Ñ‚Ð¸ ÐºÑÑˆ)")

if not selected:
    st.info("Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ñ…Ð¾Ñ‚Ñ Ð±Ñ‹ Ð¾Ð´Ð½Ñƒ Ð»ÐµÐ½Ñ‚Ñƒ ÑÐ»ÐµÐ²Ð°.")
    st.stop()

subset = {name: FEEDS[name] for name in selected}

# â€“â€“â€“â€“â€“ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° â€“â€“â€“â€“â€“
with st.spinner("Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð½Ð¾Ð²Ð¾ÑÑ‚Ð¸..."):
    if refresh:
        st.cache_data.clear()
    data = load_feeds(subset)

# â€“â€“â€“â€“â€“ Ð²Ñ‹Ð²Ð¾Ð´ â€“â€“â€“â€“â€“
for name, items in data.items():
    st.subheader(f"ðŸ—žï¸ {name}  â€”  {len(items)} ÑÑ‚Ð°Ñ‚ÐµÐ¹")
    if items and items[0].get("link") == "":
        st.error(items[0]["title"])           # ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¾Ð± Ð¾ÑˆÐ¸Ð±ÐºÐµ
        continue

    for art in items[:n_show]:
        published = f" ({art['published']})" if art["published"] else ""
        st.markdown(f"- [{art['title']}]({art['link']}){published}")

    if len(items) > n_show:
        with st.expander("ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ Ð±Ð¾Ð»ÑŒÑˆÐµ"):
            for art in items[n_show:]:
                published = f" ({art['published']})" if art["published"] else ""
                st.markdown(f"- [{art['title']}]({art['link']}){published}")