# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  news_aggregator.py   (v3: run-button, clean UI, minor fixes)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, re, time, pickle, hashlib, logging, pathlib, html
from datetime import datetime, timezone
from collections import defaultdict

import requests, feedparser, streamlit as st
from bs4 import BeautifulSoup
from deep_translator import GoogleTranslator
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# â”€â”€â”€ RSS-Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
NEWS_SOURCES = {
    "BBC News"        : "https://feeds.bbci.co.uk/news/world/rss.xml",
    "Reuters"         : "https://feeds.reuters.com/reuters/worldNews",
    "Al Jazeera"      : "https://www.aljazeera.com/xml/rss/all.xml",
    "Jerusalem Post"  : "https://www.jpost.com/rss/rssfeedsfrontpage.aspx",
    "Haaretz"         : "https://rsshub.app/haaretz/english",
    "Times of Israel" : "https://www.timesofisrael.com/feed/",
    "Kyiv Independent": "https://kyivindependent.com/feed/",
    "Ukrinform"       : "https://www.ukrinform.net/block-lastnews?format=feed",
    "CNN World"       : "https://rss.cnn.com/rss/edition_world.rss",
    "Guardian World"  : "https://www.theguardian.com/world/rss",
    "Associated Press": "https://apnews.com/hub/ap-top-news?outputType=rss",
    "DW"              : "https://rss.dw.com/rdf/rss-en-all",
    "Sky News"        : "https://feeds.skynews.com/feeds/rss/world.xml",
}

# â”€â”€â”€ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° (Ñ‚ĞµĞ³Ğ¸) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KEYWORDS = [
    "israel", "israeli", "gaza", "hamas", "idf",
    "ukraine", "russia", "russian", "war", "invasion",
    "Ğ¸Ğ·Ñ€Ğ°Ğ¸Ğ»ÑŒ", "Ğ¸Ğ·Ñ€Ğ°Ğ¸Ğ»ÑŒÑĞºĞ¸Ğ¹", "Ğ³Ğ°Ğ·Ğ°", "Ñ…Ğ°Ğ¼Ğ°Ñ",
    "ÑƒĞºÑ€Ğ°Ğ¸Ğ½Ğ°", "Ñ€Ğ¾ÑÑĞ¸Ñ", "Ğ²Ğ¾Ğ¹Ğ½Ğ°", "Ğ²Ñ‚Ğ¾Ñ€Ğ¶ĞµĞ½Ğ¸Ğµ",
]
_PATTERNS = [(kw, re.compile(rf"\b{re.escape(kw)}\b", re.I)) for kw in KEYWORDS]

def find_tags(*texts: str) -> set[str]:
    joined = " ".join(texts)
    return {kw for kw, pat in _PATTERNS if pat.search(joined)}

# â”€â”€â”€ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸/Ğ¿ÑƒÑ‚Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TARGET_LANG = "ru"
RAW_TTL     = 15 * 60                       # 15 Ğ¼Ğ¸Ğ½ÑƒÑ‚
LOG_FILE    = "news_debug.log"
CACHE_DIR   = pathlib.Path(".rss_cache"); CACHE_DIR.mkdir(exist_ok=True)

# â”€â”€â”€ Ğ»Ğ¾Ğ³Ğ³ĞµÑ€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def init_logger(path):
    log = logging.getLogger("NewsAgg")
    if not log.handlers:
        log.setLevel(logging.DEBUG)
        fh = logging.FileHandler(path, encoding="utf-8")
        fh.setFormatter(logging.Formatter("%(asctime)s | %(levelname)s | %(message)s"))
        log.addHandler(fh)
    return log
logger = init_logger(LOG_FILE)

# â”€â”€â”€ Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ñ ĞºĞµÑˆĞµĞ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=24*3600, show_spinner=False)
def translate_cached(txt, target):
    return GoogleTranslator(source="auto", target=target).translate(txt)

def translate(txt: str, enabled=True):
    if not enabled or not txt:
        return txt
    try:
        # Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ¼ 4500 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ°Ğ¼Ğ¸ (Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ Google â‰ˆ5000)
        return translate_cached(txt[:4500], TARGET_LANG)
    except Exception as e:
        logger.error("Translate error: %s", e)
        return txt

# â”€â”€â”€ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def clean_html(raw: str) -> str:
    if not raw:
        return ""
    txt = BeautifulSoup(raw, "lxml").get_text(" ", strip=True)
    txt = html.unescape(txt)
    return re.sub(r"\s{2,}", " ", txt).strip()

def first_image(entry) -> str | None:
    for attr in ("media_content", "media_thumbnail"):
        if attr in entry:
            for m in entry[attr]:
                if "url" in m:
                    return m["url"]
    for link in entry.get("links", []):
        if link.get("type", "").startswith("image"):
            return link.get("href")
    if "summary" in entry:
        m = re.search(r'<img .*?src="([^"]+)"', entry.summary, re.I)
        if m:
            return m.group(1)
    return None

# â”€â”€â”€ HTTP-ÑĞµÑÑĞ¸Ñ Ñ retry & backoff â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def make_session() -> requests.Session:
    sess = requests.Session()
    retry = Retry(
        total=4, backoff_factor=1,
        status_forcelist=[401,403,404,429,500,502,503,504],
        allowed_methods=("GET", "HEAD")
    )
    adapter = HTTPAdapter(max_retries=retry)
    sess.mount("http://",  adapter)
    sess.mount("https://", adapter)
    return sess
SESSION = make_session()

SPECIAL_HEADERS = {
    "Times of Israel": {
        "user-agent":
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.4 Safari/605.1.15",
        "accept": "application/rss+xml,application/xml;q=0.9,*/*;q=0.8",
    },
}

def proxy_url(orig: str) -> str:
    return f"https://r.jina.ai/http://{orig}"

# â”€â”€â”€ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° RSS Ñ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²Ñ‹Ğ¼ ĞºĞµÑˆĞµĞ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_raw_rss(url: str) -> bytes | None:
    fn = CACHE_DIR / (hashlib.md5(url.encode()).hexdigest() + ".pkl")
    now = time.time()

    try:
        if fn.exists():
            ts, data = pickle.loads(fn.read_bytes())
            if now - ts < RAW_TTL:
                return data

        src_name = next((k for k, v in NEWS_SOURCES.items() if v == url), "")
        headers  = {"user-agent":
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) "
                    "Gecko/20100101 Firefox/123.0"}
        headers.update(SPECIAL_HEADERS.get(src_name, {}))

        verify = not url.startswith("https://www.aljazeera.com")

        try:
            resp = SESSION.get(url, headers=headers, timeout=20, verify=verify)
            resp.raise_for_status()
        except requests.exceptions.HTTPError:
            p_url = proxy_url(url)
            logger.warning("Retry via proxy: %s", p_url)
            resp = SESSION.get(p_url, headers=headers, timeout=20)
            resp.raise_for_status()

        data = resp.content
        fn.write_bytes(pickle.dumps((now, data)))
        return data

    except Exception as e:
        logger.error("HTTP error %s â†’ %s", url, e)
        return None

# â”€â”€â”€ ÑĞ±Ğ¾Ñ€ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def collect_news(need_translate: bool):
    news, stats = [], defaultdict(lambda: {"total": 0, "matched": 0})
    seen = set()

    for src, url in NEWS_SOURCES.items():
        xml = load_raw_rss(url)
        if xml is None:
            continue

        feed = feedparser.parse(xml)
        stats[src]["total"] = len(feed.entries)
        logger.info("%s â†’ %s entries", src, len(feed.entries))

        for e in feed.entries:
            link_clean = e.link.split("?", 1)[0].split("#", 1)[0]
            if link_clean in seen:
                continue
            seen.add(link_clean)

            title_o = e.title
            desc_o  = clean_html(getattr(e, "summary", "") or getattr(e, "description", ""))

            tags = find_tags(title_o, desc_o)
            if not tags:
                continue
            stats[src]["matched"] += 1

            title_t = translate(title_o, need_translate)
            desc_t  = translate(desc_o, need_translate)
            tags |= find_tags(title_t, desc_t)

            if getattr(e, "published_parsed", None):
                ts = time.mktime(e.published_parsed)
                dt = datetime.fromtimestamp(ts, tz=timezone.utc)
            else:
                dt = datetime.utcnow().replace(tzinfo=timezone.utc)

            news.append({
                "title"     : title_t,
                "orig_title": title_o,
                "desc"      : desc_t,
                "orig_desc" : desc_o,
                "image"     : first_image(e),
                "link"      : link_clean,
                "source"    : src,
                "time"      : dt,
                "tags"      : sorted(tags),
            })

    news.sort(key=lambda x: x["time"], reverse=True)
    return news, stats

# â”€â”€â”€ UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config("News Aggregator", layout="wide")
st.title("ğŸŒ ĞĞ³Ñ€ĞµĞ³Ğ°Ñ‚Ğ¾Ñ€ Ğ¼ĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹")

# --- SIDEBAR ---
with st.sidebar:
    st.header("âš™ï¸ ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹")

    # Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´
    st.checkbox("ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ (ru)", True, key="tr_flag")

    # Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ (Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ¾ Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ)
    selected_sources = st.multiselect(
        "Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸:",
        options=list(NEWS_SOURCES.keys()),
        default=[],
        help="ĞĞ°Ğ¶Ğ¼Ğ¸Ñ‚Ğµ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ»ĞµĞ½Ñ‚Ñ‹"
    )

    # Ñ‚ĞµĞ³Ğ¸
    selected_tags = st.multiselect(
        "Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ¿Ğ¾ Ñ‚ĞµĞ³Ğ°Ğ¼:",
        options=sorted(set(KEYWORDS)),
        default=[],
        help="Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)"
    )
    filter_mode = st.radio(
        "Ğ ĞµĞ¶Ğ¸Ğ¼ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°:",
        ["OR (Ğ»ÑĞ±Ğ¾Ğ¹ Ñ‚ĞµĞ³)", "AND (Ğ²ÑĞµ Ñ‚ĞµĞ³Ğ¸)"],
        horizontal=True,
        key="filter_mode"
    )

    st.markdown("---")
    # run-button
    run_btn = st.button("ğŸš€ Ğ¡Ğ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸")

    # Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ĞºĞ½Ğ¾Ğ¿ĞºĞ° Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ ĞºĞµÑˆĞ°
    if st.button("â™»ï¸ ĞÑ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ ĞºĞµÑˆ + Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑĞº"):
        for f in CACHE_DIR.glob("*.pkl"):
            f.unlink(missing_ok=True)
        open(LOG_FILE, "w", encoding="utf-8").close()
        st.session_state.clear()
        st.experimental_rerun()

# --- Ğ›ĞĞ“Ğ˜ĞšĞ Ğ—ĞĞŸĞ£Ğ¡ĞšĞ ---
translate_flag = st.session_state.get("tr_flag", True)

# Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¼ Ğ·Ğ°Ñ…Ğ¾Ğ´Ğµ, Ğ±Ğ»Ğ¾ĞºĞ¸ Ğ² session_state Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼
st.session_state.setdefault("_all_news", [])
st.session_state.setdefault("_all_stats", {})
st.session_state.setdefault("_filters", {})

if run_btn:
    # Ğ·Ğ°Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ²ÑĞµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸
    news_all, stats_all = collect_news(translate_flag)

    # ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² ÑĞµÑÑĞ¸Ñ
    st.session_state["_all_news"]  = news_all
    st.session_state["_all_stats"] = stats_all
    st.session_state["_filters"]   = {
        "sources": selected_sources,
        "tags"   : selected_tags,
        "mode"   : filter_mode,
    }

# --- Ğ’Ğ«Ğ’ĞĞ” Ğ¡ Ğ ĞĞĞ•Ğ• Ğ¡ĞĞ‘Ğ ĞĞĞĞ«ĞœĞ˜ Ğ”ĞĞĞĞ«ĞœĞ˜ ---
if not st.session_state["_all_news"]:
    st.info("Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ²Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¸ Ğ½Ğ°Ğ¶Ğ¼Ğ¸Ñ‚Ğµ Â«Ğ¡Ğ¾Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸Â».")
    st.stop()

news  = st.session_state["_all_news"]
stats = st.session_state["_all_stats"]
fset  = st.session_state["_filters"]

# Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼
if fset["sources"]:
    news  = [n for n in news if n["source"] in fset["sources"]]
    stats = {s: d for s, d in stats.items() if s in fset["sources"]}

# Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ñ‚ĞµĞ³Ğ°Ğ¼
if fset["tags"]:
    if fset["mode"].startswith("OR"):
        news = [n for n in news if set(n["tags"]) & set(fset["tags"])]
    else:
        news = [n for n in news if set(fset["tags"]).issubset(n["tags"])]

# --- Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ ---
st.subheader("ğŸ“Š Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ¿Ğ¾ Ğ»ĞµĞ½Ñ‚Ğ°Ğ¼")
st.dataframe(
    [{"Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº": s, "Ğ’ÑĞµĞ³Ğ¾ ÑÑ‚Ğ°Ñ‚ĞµĞ¹": d["total"], "Ğ¡Ğ¾Ğ²Ğ¿Ğ°Ğ»Ğ¾": d["matched"]}
     for s, d in stats.items()],
    use_container_width=True
)

# --- Ğ’Ğ«Ğ’ĞĞ” ĞĞĞ’ĞĞ¡Ğ¢Ğ•Ğ™ ---
if not news:
    st.warning("ĞĞ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ¿Ğ¾Ğ´ Ğ²Ñ‹Ğ±Ñ€Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹.")
else:
    st.success(f"ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ĞµĞ¹: {len(news)}")

    for n in news:
        st.subheader(n["title"])

        cols = st.columns([2, 5])
        with cols[0]:
            if n["image"]:
                st.image(n["image"], use_column_width=True)
        with cols[1]:
            st.write(f"**Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº:** {n['source']}")
            if n["time"]:
                st.write(f"**Ğ”Ğ°Ñ‚Ğ° (UTC):** {n['time'].strftime('%Y-%m-%d %H:%M')}")
            st.markdown(f"[Ğ§Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ»ĞµĞµ]({n['link']})")
            if n["tags"]:
                tag_str = "  ".join(f":blue[{t}]" for t in n["tags"])
                st.write(tag_str)

        if n["desc"]:
            st.write(n["desc"])

        with st.expander("ĞŸĞ¾Ğ´Ñ€Ğ¾Ğ±Ğ½ĞµĞµ / Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»"):
            if n["orig_desc"]:
                st.write(n["orig_desc"])
            st.write(f"ĞÑ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº: _{n['orig_title']}_")

        st.divider()